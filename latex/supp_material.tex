%!TEX spellcheck
\documentclass[10pt]{article}
\renewcommand{\baselinestretch}{1.5} 
% \documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage{amscd, amsfonts, amsmath, amssymb, amstext, amsthm, caption, epsfig, fancyhdr, float, graphicx, latexsym, mathtools, multicol, multirow, algorithm, chngcntr}
\usepackage{kpfonts}
\usepackage[lofdepth,lotdepth]{subfig}
% \usepackage{subcaption}
\usepackage{authblk}
\usepackage[utf8]{inputenc}
\usepackage{bm} 
\usepackage{enumerate}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage[numbers]{natbib}
\usepackage{color}
\usepackage{bm}
\usepackage{parskip}

\usepackage[pdfpagelabels,colorlinks=true,linkcolor=blue,anchorcolor=blue,citecolor=blue,filecolor=blue,menucolor=blue,runcolor=blue,urlcolor=blue]{hyperref}

\usepackage{algpseudocode} % Loading after hyperref

% For PGF plots
\usepackage{pgf}
\usepackage[utf8]{inputenc}\DeclareUnicodeCharacter{2212}{-}

\usepackage{caption}
\captionsetup{skip=0.5em}

\usepackage[capitalise,nameinlink,noabbrev]{cleveref}
\crefname{equation}{}{}
\crefname{enumi}{}{}

% Load images from subdirectory
\graphicspath{{Figures/}}

% To fix some warnings: https://tex.stackexchange.com/questions/177025/hyperref-cleveref-and-algpseudocode-same-identifier-warning
\makeatletter
\newcounter{algorithmicH}% New algorithmic-like hyperref counter
\let\oldalgorithmic\algorithmic
\renewcommand{\algorithmic}{%
  \stepcounter{algorithmicH}% Step counter
  \oldalgorithmic}% Do what was always done with algorithmic environment
\renewcommand{\theHALG@line}{ALG@line.\thealgorithmicH.\arabic{ALG@line}}
\makeatother

% \newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\makeatletter
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
\makeatletter

% https://tex.stackexchange.com/questions/76273/multiple-pdfs-with-page-group-included-in-a-single-page-warning
\pdfsuppresswarningpagegroup=1

% \newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\makeatletter
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\bm{\theta}ble}
\makeatletter

% % only equations which are labeled AND referenced will be numbered.
% \usepackage{autonum}

\newtheorem{lemma}{Lemma}
\newtheorem{theo}{Theorem}
\newtheorem{coro}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{property}{Property}
\newtheorem{remark}{Remark}[section]
\newtheorem{ex}{Example}
\newtheorem{definition}{Definition}
\newtheorem{hp}{Assumption}
\newtheorem{pb}{Problem}

% Distributions.
\newcommand*{\UnifDist}{\mathsf{Unif}}
\newcommand*{\ExpDist}{\mathsf{Exp}}
\newcommand*{\DepExpDist}{\mathsf{DepExp}}
\newcommand*{\GammaDist}{\mathsf{Gamma}}
\newcommand*{\LognormalDist}{\mathsf{LogNorm}}
\newcommand*{\WeibullDist}{\mathsf{Weib}}
\newcommand*{\ParetoDist}{\mathsf{Par}}
\newcommand*{\NormalDist}{\mathsf{Normal}}

\newcommand*{\GeometricDist}{\mathsf{Geom}}
\newcommand*{\BernouilliDist}{\mathsf{Ber}}
\newcommand*{\NegBinomialDist}{\mathsf{NegBin}}
\newcommand*{\BinomialDist}{\mathsf{Bin}}
\newcommand*{\PoissonDist}{\mathsf{Poisson}}
\newcommand*{\BivariatePoissonDist}{\mathsf{BPoisson}}
\newcommand*{\CyclicalPoissonDist}{\mathsf{CPoisson}}

% Sets of numbers.
\newcommand*{\RL}{\mathbb{R}}
\newcommand*{\NZ}{\mathbb{N}_0}
% \newcommand*{\NL}{\mathbb{N}_+}

\newcommand*{\cond}{\mid}
\newcommand*{\given}{\,;\,}

% Regarding spacing and abbreviations.
\usepackage{xspace}

% Acronyms
% \@\xspace doesn't add space if next char is punctuation
% However, these will give 2 .'s if used at end of sentence.
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}

\newcommand*{\iid}{\textsc{iid}\@\xspace}
\newcommand*{\pdf}{\textsc{pdf}\@\xspace}
\newcommand*{\pmf}{\textsc{pmf}\@\xspace}
\newcommand*{\abc}{{\textsc{abc}}\@\xspace}
\newcommand*{\iso}{{\textsc{iso}}\@\xspace}
\newcommand*{\smc}{\textsc{smc}\@\xspace}
\newcommand*{\mcmc}{\textsc{mcmc}\@\xspace}
\newcommand*{\ess}{\textsc{ess}\@\xspace}
\newcommand*{\mle}{\textsc{mle}\@\xspace}
\newcommand*{\bic}{\textsc{bic}\@\xspace}
\newcommand*{\kde}{\textsc{kde}\@\xspace}
\newcommand*{\glm}{\textsc{glm}\@\xspace}
\newcommand*{\xol}{\textsc{xol}\@\xspace}
\newcommand*{\cpu}{\textsc{cpu}\@\xspace}
\newcommand*{\gpu}{\textsc{gpu}\@\xspace}
\newcommand*{\arm}{\textsc{arm}\@\xspace}
\newcommand*{\map}{\textsc{map}\@\xspace}
\newcommand*{\bp}{\textsc{bp}\@\xspace}
\newcommand*{\mode}{\textsc{mode}\@\xspace}

\newcommand*{\iidSim}{\overset{\text{\iid}}{\sim}}
\newcommand*{\bt}{\bm{\bm{\theta}}}
\newcommand*{\bTheta}{\bm{\bm{\theta}}}
\newcommand*{\bbeta}{\bm{\beta}}
\newcommand*{\bn}{\bm{n}}
\newcommand*{\bs}{\bm{s}}
\newcommand*{\bu}{\bm{u}}
\newcommand*{\bv}{\bm{v}}
\newcommand*{\bx}{\bm{x}}
\newcommand*{\by}{\bm{y}}
\newcommand*{\bH}{\bm{H}}

% Roman versions of things.
\newcommand*{\dd}{\mathop{}\!\mathrm{d}}
\newcommand*{\e}{\mathrm{e}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand*{\norm}[1]{\lVert{} #1\rVert}
\newcommand*{\Dis}{\mathcal{D}}
\newcommand*{\Was}{\mathcal{W}}
\newcommand*{\argdot}{\,\cdot\,}
\newcommand*{\Oh}{\mathcal{O}} % Big-O notation

% \DeclarePairedDelimiterXPP{\ind}[1]{\ind_}{\{}{\}}{}{#1}
\newcommand*{\ind}{\mathbb{I}}

% The specific colours used in our figures.
\definecolor{MyBlue}{HTML}{1f77b4}
\definecolor{MyGreen}{HTML}{2ca02c}
\definecolor{MyRed}{HTML}{d62728}
\definecolor{MyPurple}{HTML}{9467bd}
\definecolor{MyBrown}{HTML}{8c564b}

% Commands to have coloured text in the captions.
\newcommand*{\CapBlue}[1]{\textcolor{MyBlue}{\textbf{#1}}}
\newcommand*{\CapGreen}[1]{\textcolor{MyGreen}{\textbf{#1}}}
\newcommand*{\CapRed}[1]{\textcolor{MyRed}{\textbf{#1}}}
\newcommand*{\CapPurple}[1]{\textcolor{MyPurple}{\textbf{#1}}}
\newcommand*{\CapBrown}[1]{\textcolor{MyBrown}{\textbf{#1}}}
\newcommand*{\CapBlack}[1]{\textbf{#1}}

\usepackage[percent]{overpic}
\newcommand{\float}[1]{\makebox[0pt]{#1}}
\newcommand{\subfig}[1]{\float{\Large #1}}
\newcommand{\subfigsmall}[1]{\float{#1}}

\usepackage{float}
\usepackage{microtype}

\renewcommand{\tilde}{\widetilde}
\renewcommand{\hat}{\widehat}

\usepackage{cancel}

\renewcommand*{\bibfont}{\small}
\usepackage[marginratio=1:1,height=584pt,width=480pt,tmargin=90pt]{geometry}

\begin{document}

\title{Online supplement for "Market-based insurance ratemaking: application to pet insurance"}
\author[1]{Pierre-Olivier Goffard \footnote{Email: \href{mailto:goffard@unistra.fr}{goffard@unistra.fr}.}}
\author[2,3]{Pierrick Piette\footnote{Email: \href{mailto:pierrick.piette@gmail.com}{pierrick.piette@gmail.com}.}}
\author[4]{Gareth W. Peters\footnote{Email: \href{mailto:garethpeters@ucsb.edu}{garethpeters@ucsb.edu}.}}
\affil[1]{\footnotesize Université de Strasbourg, Institut de Recherche Mathématique Avancée, Strasbourg, France}
\affil[2]{\footnotesize Univ Lyon, Université Claude Bernard Lyon 1, Institut de Science Financière et d’Assurances (ISFA), Laboratoire SAF EA2429, F-69366, Lyon, France}
\affil[3]{\footnotesize  Seyna, 10 Rue du Faubourg Montmartre 75009 Paris}
\affil[4]{\footnotesize  University of California Santa Barbara, Department of Statistics and Applied Probability, Santa Barbara CA 93106-3110, USA}

\maketitle
\vspace{3mm}
The goal of this supplementary material is to discuss the identifiability issue associated to finding the parameters of the risk model. Let $X$ be a random variable that represents the total health expenses over a given time period, say one year, associated to a pet. Our goal is to find the parameter $\theta$ that best explains our market data made of insurance quotes
\begin{equation}\label{eq:commercial_premiums}
\tilde{p}_i = f_i\left[\mathbb{E}(g_i(X))\right],\text{ }i = 1,\ldots, n,
\end{equation}
where $g_i$ are the coverage functions and $f_i$ are the loading function. The coverage functions are known and of the form
$$
g(x) = \min(\max(r\cdot x - d, 0), l),
$$
where $r$ is the rate of coverage, $d$ is the deductible and $l$ is the limit. The loading functions $f_i$ are unknown and will be approximated by a generic function $f$. 

In \cref{sec:problem1}, we consider the problem of finding $\theta$ if we know the pure premiums 
$$
p_i = \left[\mathbb{E}(g_i(X))\right],\text{ }i = 1,\ldots, n,
$$
which is not a realistic situation in practice. We look into the actual problem, relying on the commercial premiums \eqref{eq:commercial_premiums}, in \cref{sec:problem2}. An isotonic regression model is used to approximate $f$. This choice  is compared to a linear fit $f(x) =b\cdot x$. We consider the model
$$
X = \sum_{k=1}^NU_k,
$$
where $N\sim\text{Pois}(\lambda = 3)$ and the $U_k$ are \iid and lognormally distributed $U\sim\text{LogNormal}(\mu =0 , \sigma = 1)$. Our aim is to estimate the parameters $\theta = (\begin{array}{ccc}\lambda& \mu & \sigma\end{array})$ based on three insurance quotes associated to the following coverage functions
$$
g_1 = 0.85\cdot X\text{, } g_2 = \max(X - 1.8, 0) \text{, and }g_3 = \min(X, 6).
$$
In terms of coverage rate, deductible and limit, it is equivalent to $(r_1 = 0.85, d_1 = 0, l_1 = \infty)$, $(r_2 = 1, d_2 = 1.8, l_2 = \infty)$, and $(r_3 = 1, d_3 = 0, l_3 = 6)$ respectively. The pure premium associated to these coverages are provided in \cref{tab:true_pure_premium}.

\begin{table}[ht]
\centering
\begin{tabular}{llllr}
  \toprule
r & d & l &\phantom{abc}& Pure premium \\ 
  \midrule
0.75 & 0.00 & Inf && 3.71 \\ 
  1.00 & 1.80 & Inf && 3.40 \\ 
  1.00 & 0.00 & 6.00 && 3.63 \\ 
   \bottomrule
\end{tabular}
\caption{Pure premium associated to $g_1, g_2$ and $g_3$ of the $\text{Pois}(\lambda = 3)-\text{LogNormal}(\mu =0 , \sigma = 1)$ risk model.}
\label{tab:true_pure_premium}
\end{table}


\section{Optimization problem 1}\label{sec:problem1}
The first optimization that we consider reads as follows:
\begin{pb}\label{pb:optimization_problem_simple}
Find $\theta\in \Theta\subset\mathbb{R}^d$ to minimize \(d\left[p_{1:n}, p_{1:n}^\theta\right]\), where 
$$
p_i^\theta =\mathbb{E}_\theta\left[g_{i}(X)\right],\text{ for }i = 1,\ldots, n,
$$
are the pure premium associated to $X$ parametrized by $\theta\in \Theta\subset\mathbb{R}^d$ and $d(\cdot, \cdot)$ denotes a distance function over the observation space. 
\end{pb}
Wew measure the discrepancy between observed and model-generated pure premiums using the root mean square error (RMSE) defined as
\begin{equation}\label{eq:sse_distance_pure}
\text{RMSE}\left[p_{1:n}, p^\theta_{1:n}\right] = \sqrt{\sum_{i=1}^nw^{\text{RMSE}}_i [p_i - p_i^\theta]^2}.
\end{equation}
The statistical framework is that of minimum distance estimation. We do not have access to the full shape of the data distribution. We must base our inference on specific moments, just as in the generalized method of moments, a popular method among econometricians (see \citet{Hansen1982}). The model is identifiable if there exists a unique $\theta^\ast$ such that 
\begin{equation}\label{eq:optimization_knowing_pure_premium}
\theta^\ast  = \underset{\theta\in \Theta}{\argmin}\,\text{RMSE}(p_{1:n}, p_{1:n}^\theta).
\end{equation}
Existence stems from the fact that the parameter space $\Theta$ is compact and the map $\theta\mapsto\text{RMSE}(p_{1:n}, p_{1:n}^\theta)$ is continuous. Uniqueness is more difficult to verify as it depends on the functional $g_i$'s. Given the model for $X$ and the insurance coverages, the pure premium do not have an analytical expression making it difficult to show the convexity of \eqref{eq:sse_distance_pure}. A simple necesssary condition is that the number of parameters must be smaller than $n$, the number of moments considered. We consider here a situation where we have only three pure premiums $(n=3)$ and we know that 
$$
\text{RMSE}\left[p_{1:n}, p^{\theta^\ast}_{1:n}\right] = 0, 
$$
% for $\theta^\ast = (\begin{array}{ccc}\lambda = 3& \mu = 0 & \sigma = 1\end{array})$. The problem is that there might exist another combination of paremeters that yield exactly the same pure premium. It is difficult to investigate this issue from a theoretical point of view, we provide here an empirical approach. 
\cref{fig:rmse_pure_premium} shows the plot of the functions 
$$
\lambda\mapsto d\left(p_{1:n},p_{1:n}^{\theta}\right)\Big\rvert_{\mu =0, \sigma = 1}\text{, }\mu\mapsto d\left(p_{1:n},p_{1:n}^{\theta}\right)\Big\rvert_{\sigma =1, \lambda = 3}\text{, and }\sigma\mapsto d\left(p_{1:n},p_{1:n}^{\theta}\right)\big\rvert_{\mu =0, \lambda = 3}.
$$

\begin{figure}[!ht]
  \begin{center}
    \subfloat[$\lambda\mapsto d\left(p_{1:n},p_{1:n}^{\theta}\right)\Big\rvert_{\mu =0, \sigma = 1}$]{
      \includegraphics[width=0.4\textwidth]{../figures/lambda_pure_RMSE}
      \label{sub:lambda_pure_RMSE}
    }
    \subfloat[$\mu\mapsto d\left(p_{1:n},p_{1:n}^{\theta}\right)\Big\rvert_{\sigma =1, \lambda = 3}$]{
      \includegraphics[width=0.4\textwidth]{../figures/mu_pure_RMSE}
      \label{sub:mu_pure_RMSE}
    }\\
    \subfloat[$\sigma\mapsto d\left(p_{1:n},p_{1:n}^{\theta}\right)\big\rvert_{\mu =0, \lambda = 3}$
    ]{
      \includegraphics[width=0.4\textwidth]{../figures/sig_pure_RMSE}
      \label{sub:sig_pure_RMSE}
    }
    \caption{Plots of the RMSE when looking into each parameter separately.}
    \label{fig:rmse_pure_premium}
  \end{center}
\end{figure}

By examining the parameters individually, the model appears to be identifiable. However, this does not imply that a unique parametrization can be determined unless two out of the three parameters are fixed. A grid search procedure is not feasible for exploring the three-dimensional parameter space. Instead, we employ the optimization procedure described in Section 3.3 of our paper. The parameters of the algorithm are configured as follows:  
$$
J = 500, \quad R = 500, \quad \text{and} \quad \epsilon_{\min} = 0.02.
$$
Here, $J$ represents the population size of the clouds of particles, $R$ is the number of Monte Carlo replications, and $\epsilon_{\min}$ is the threshold for the tolerance level. The algorithm terminates once the tolerance threshold reaches $\epsilon_{\min}$. The choice of the threshold $\epsilon_{\min} = 0.02$ is based on an estimation of the error in the RMSE resulting from using a Monte Carlo approximation to compute the pure premium. The prior distributions of the parameters are defined as:  
$$
\lambda \sim \UnifDist([0, 10]), \quad \mu \sim \UnifDist([-3, 3]), \quad \text{and} \quad \sigma \sim \UnifDist([0, 2]).
$$
\cref{fig:posterior_pure_premium} displays the resulting posterior distributions.

\begin{figure}[!ht]
  \begin{center}
    \subfloat[Posterior distribution of $\lambda$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_lambda_supp_mat}
      \label{sub:posterior_lambda_supp_mat}
    }
    \subfloat[Posterior distribution of $\mu$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_mu_supp_mat}
      \label{sub:posterior_mu_supp_mat}
    }\\
    \subfloat[Posterior distribution of $\sigma$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_sig_supp_mat}
      \label{sub:posterior_sig_supp_mat}
    }
    \caption{Posterior distributions of the parameters of the $\text{Pois}(\lambda)-\text{LogNormal}(\mu , \sigma)$ risk model.}
    \label{fig:posterior_pure_premium}
  \end{center}
\end{figure}

If the pure premium could be calculated exactly, our algorithm would yield Dirac distributions concentrated at the true parameter values. However, since we rely on a Monte Carlo approximation, the use of an Approximate Bayesian Computation (ABC) procedure is justified, as it incorporates the uncertainty introduced by the Monte Carlo error into the posterior distribution. in view of \cref{fig:posterior_pure_premium}, we conclude that having access to the pure premium enables the identification of the true parameters of the model. The situation becomes more challenging when only the commercial premium is available, as will be discussed in the next section.

\section{Optimization problem 2}\label{sec:problem2}

As mentionned earlier, we do not have access to the pure premiums. Instead we have a collection of commercial rates $\Tilde{p}_{1:n} = \{\Tilde{p}_{1},\ldots, \Tilde{p}_{n}\}$ defined as
$$
\Tilde{p}_i = f_i(p_i) = f_i\left\{\mathbb{E}_{\theta_0}\left[g_{i}(X)\right]\right\},\text{ }i = 1,\ldots, n.
$$
It leads us to formulate the following optimization problem: 
\begin{pb}\label{pb:optimization_problem}
Find $\theta\in \Theta\subset\mathbb{R}^d$ and $f:\mathbb{R}_+\mapsto \mathbb{R}_+$ to minimize \(d\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right]\), where the function $f$ is applied elementwise on $p_{1:n}^\theta$ and $d(\cdot, \cdot)$ denotes a distance function over the observation space, subject to 
\begin{equation}\label{eq:constraint}
\Tilde{p}_i \geq p_i^\theta,\text{ and } f(p_i^\theta) \geq p_i^\theta, \text{for }i = 1,\ldots, n.
\end{equation} 
\end{pb}

Our first task is to find a generic function $f$ to represent the safety loading functions $f_i$'s used by the competitors. For this, we use isotonic regression. It is a statistical technique used for fitting a non-decreasing function to a set of data points. The idea is that if two pure premium satisfy $p_i\leq p_j$ then the commercial premium should also verify $\Tilde{p}_i\leq \Tilde{p}_j$. Consider a collection of candidate pure premiums $p_{1:n}^\theta$, associated to a candidate risk parameter $\theta$. Our datapoints are therefore pairs of pure and commercial premiums $(p_i^\theta,\Tilde{p}_i)_{i=1,\ldots, n}$. Suppose the pure premium have been ordered such that $p_i^\theta\leq p_j^\theta$ for $i\leq j$, isotonic regression seeks a least square fit $\Tilde{p}^\theta_i$ for the $\Tilde{p}_i$'s such that $\Tilde{p}^\theta_i\leq \Tilde{p}^\theta_j$ for $p_i^\theta\leq p_j^\theta$. It reduces to find $\Tilde{p}^\theta_1,\ldots, \Tilde{p}^\theta_n$ that minimize
$$
\sum_{i=1}^nw_i^{\textsc{iso}}(\Tilde{p}^\theta_i- \Tilde{p}_i)^2\text{, subject to }\Tilde{p}_i^\theta\leq \Tilde{p}^\theta_j\text{ whenever }p_i^\theta\leq p_j^\theta,
$$
where $\left(w_i^{\textsc{iso}}\right)_{i = 1, \ldots, n}$ denotes the weights associated to each pair $(p_i^\theta,\Tilde{p}_i)_{i=1,\ldots, n}$. Since the $p_i^\theta$'s fall in a totally ordered space, a simple iterative procedure called the Pool Adjacent Violators Algorithm (PAVA) can be used. Here's a high-level overview of how it works:
\begin{enumerate}
\item Initialize the sequence of values to be the same as the data points $\Tilde{p}_i^\ast =\Tilde{p}_i$.

\item Iterate through the sequence and identify "violations," which occur when the current value is greater than the next value, that is 
$$
\Tilde{p}_i^\ast>\Tilde{p}_{i+1}^\ast\text{ for some }i=1,\ldots, n.
$$ 
When a violation is found, adjust the values in the associated segment of the sequence to be the average of the values,
$$
\Tilde{p}_i^\ast \leftarrow (\Tilde{p}_i^\ast + \Tilde{p}_{i+1}^\ast)/2,
$$ 
ensuring monotonicity.
\item Repeat Step 2 until no violations are left. 
\end{enumerate}
We use the \texttt{isoreg} function from $R$ to get the fitted values $\Tilde{p}^\theta_i,\text{ }i=1,\ldots, n$. To complete the isotonic regression task we shall find a function $f$ such that $f(p_i^\theta)=\Tilde{p}^\theta_i$. A common choice is a piece-wise constant function that interpolates the $\Tilde{p}^\theta_i$'s.\\

We now turn to the definition of a distance. Our starting point to measure the discrepancy between observed and model-generated commercial rates is the root mean square error (RMSE) defined as
\begin{equation}\label{eq:sse_distance}
\text{RMSE}\left[\Tilde{p}_{1:n}, f(p^\theta_{1:n})\right] = \sqrt{\sum_{i=1}^nw^{\text{RMSE}}_i [\Tilde{p}_i - f(p_i^\theta)]^2},
\end{equation}
for a candidate risk parameter $\theta$ and an isotonic fit $f$. We seek 
$$
\theta^\ast = \underset{\theta\in \Theta}{\argmin}\,
\text{RMSE}\left[\Tilde{p}_{1:n}, f(p^\theta_{1:n})\right].
$$

The existence of such $\theta^\ast$ is guaranteed because $\theta\mapsto \text{RMSE}\left[\Tilde{p}_{1:n}, f(p^\theta_{1:n})\right]$ only takes a finite number of values. Indeed, to each $\theta \in \Theta$ is associated a unique permutation $s^\theta\in S_n$, where $S_n$ denotes the set of all the permutations of $\{1,\ldots, n\}$, such that 
$$
p^\theta_{s^\theta(1)}\leq\ldots\leq p^\theta_{s^\theta(n)}.
$$
This permutation $s^\theta$ defines a unique isotonic fit $f$ based on 
$$
\Tilde{p}^\theta_{s^\theta(1)}\leq\ldots\leq \Tilde{p}^\theta_{s^\theta(n)},
$$
leading to a given RMSE value $\text{RMSE}\left[\Tilde{p}_{1:n}, f(p^\theta_{1:n})\right]$. Concretely, for $\theta_1,\theta_2\in \Theta$, if it holds that $s^{\theta_1} = s^{\theta_2}$ then $\text{RMSE}\left[\Tilde{p}_{1:n}, f(p^{\theta_1}_{i:n})\right] = \text{RMSE}\left[\Tilde{p}_{1:n}, f(p^{\theta_2}_{i:n})\right]$. The application $\theta\mapsto s_n^\theta$ is surjective since $S_n^\Theta = \{s_n^\theta\text{ ; }\theta\in \Theta\}$  is finite.  The fact that $\Theta$ is a continuous space implies that $\theta^\ast$ cannot be unique. Our problem is an ill-posed inverse problem. Ill-posedness is usually dealt with by adding a regularization to the objective function that one wants to minimize. The ratio of \(p / \Tilde{p}\) corresponds to what practitioners would call the expected Loss Ratio (\(\text{LR}\)). Our solution is based on targeting a given loss ratio.  The loss ratio is a standard measure to assess the profitability of insurance lines of business. An insurance company that enters a new market is likely to have insights on the loss ratio relative to this market, for example by having informal discussions with reinsurers, brokers or competitors. These insights may translate into the definition of a lower and upper bound denoted by \(\text{LR}_{\text{low}}\) and \(\text{LR}_{\text{high}}\), respectively. We can then assume that the loss ratios \(\text{LR}_i = p_i / \Tilde{p}_i\), for \(i = 1, \ldots, n\), should fall in the range \([\text{LR}_{\text{low}}, \text{LR}_{\text{high}}]\), which we refer to as the loss ratio corridor. Assuming that \(\text{LR}_{\text{high}} < 1\), we may ensure both constraints in \eqref{eq:constraint} by adding to our distance \eqref{eq:sse_distance} two regularization terms defined as
\begin{equation*}\label{eq:LR_bounds1}
\text{Reg}_{\text{low}}\left(\Tilde{p}_{1:n}, p_{1:n}^\theta\right) = \sqrt{\sum_{i=1}^nw_i^{\text{RMSE}}\left(\Tilde{p}_i - p_i^\theta \cdot \text{LR}_{\text{low}}^{-1}\right)_+^2}, 
\end{equation*}
and 
\begin{equation*}\label{eq:LR_bounds2}
\text{Reg}_{\text{high}}\left(\Tilde{p}_{1:n}, p_{1:n}^\theta\right) = \sqrt{\sum_{i=1}^nw_i^{\text{RMSE}}\left(p_i^\theta \cdot \text{LR}_{\text{high}}^{-1} - \Tilde{p}_i\right)_+^2},
\end{equation*}
where \((x)_+ = \max(x,0)\) denotes the positive part of \(x\). The distance we consider within \cref{pb:optimization_problem} is now given by 
\[
d\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right] = \text{RMSE}\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right] + \text{Reg}_{\text{low}}\left(\Tilde{p}_{1:n}, p_{1:n}^\theta\right) + \text{Reg}_{\text{high}}\left(\Tilde{p}_{1:n}, p_{1:n}^\theta\right).
\]
We take again our three insurance coverages $g_1, g_2$ and $g_3$ and we distinguish two safety loading situations. The well-specified case in \cref{ssec:well_specfifed_safety_loading} and the misspecified  in \cref{ssec:miss_specfifed_safety_loading}
\subsection{Well-specified safety loading}\label{ssec:well_specfifed_safety_loading}
Consider the following loading functions
$$
f_1(x) = 1.38\cdot x\text{, }f_2(x) = 1.1\cdot x \text{, and } f_3(x) = 1.4\cdot x. 
$$
The isotonic model is well specified in this case because the pure and commercial premium are ordered in the same way. In that case the isotonic regression exactly interpolates the datapoints as shown on \cref{fig:pp_cp_well_specified}.

\begin{figure}[!ht]
  \begin{center}
  \includegraphics[width=0.7\linewidth]{../figures/pp_cp_well_specified}
    \caption{Isotonic fit of the pure and commercial premium when the isotonic model is well-specified.}
    \label{fig:pp_cp_well_specified}
  \end{center}
\end{figure}

\cref{fig:rmse_sigma_iso_linear} shows the plot of the $\sigma\mapsto  \text{RMSE}\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right]\Big\rvert_{\lambda =3, \mu =0}$ when $f$ is isotonic (\cref{sub:rmse_sigma_iso}) and when $f$ is linear (\cref{sub:rmse_sigma_linear}).

\begin{figure}[!ht]
  \begin{center}
    \subfloat[$f$ is isotonic]{
      \includegraphics[width=0.4\textwidth]{../figures/rmse_sigma_iso}
      \label{sub:rmse_sigma_iso}
    }
    \subfloat[$f$ is linear]{
      \includegraphics[width=0.4\textwidth]{../figures/rmse_sigma_linear}
      \label{sub:rmse_sigma_linear}
    }
    \caption{$\sigma\mapsto  \text{RMSE}\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right]\Big\rvert_{\lambda =3, \mu =0}$.}
    \label{fig:rmse_sigma_iso_linear}
  \end{center}
\end{figure}

Note that in this example, we do not include the regularization terms. When \(f\) is isotonic, the values of \(\sigma\) for which the RMSE is zero correspond to the same ordering of the pure premium as that of the true parameter \(\sigma = 1\) (see \cref{sub:rmse_sigma_iso}). Consequently, several values of \(\sigma\) are optimal, including the true one. 

In contrast, when \(f\) is linear, the RMSE is minimized for the parameter value that best aligns the commercial premium. Although one value of \(\sigma\) seems to stand out, it is notably different from the true value (see \cref{sub:rmse_sigma_linear}). From \cref{sub:rmse_sigma_iso}, one might expect that running our ABC algorithm would produce a uniform posterior distribution. 

The parameters of the algorithm are set as follows:  
$$
J = 2000, \quad R = 500, \quad \text{and} \quad \epsilon_{\min} = 10^{-8}.
$$
We define a prior distribution on \(\sigma\) as \(\sigma \sim \UnifDist([0, 2])\). The tolerance level is set very low because we are specifically interested in the values of \(\sigma\) that produce the correct ordering and an RMSE close to zero. The resulting posterior distribution is shown in \cref{fig:posterior_sig_iso_supp_mat}.

\begin{figure}[!ht]
  \begin{center}
  \includegraphics[width=0.7\linewidth]{../figures/posterior_sig_iso_supp_mat}
    \caption{Intermediate posterior distributions for $\sigma$.}
    \label{fig:posterior_sig_iso_supp_mat}
  \end{center}
\end{figure}

The posterior distribution is not entirely uniform, which can be attributed to the Monte Carlo approximation used. This approximation may alter the ordering of the pure premium when approaching the edges of the admissible range of \(\sigma\) values. The posterior distribution shown in \cref{fig:posterior_sig_iso_supp_mat} might appear satisfactory to a naive observer, leading one to argue that regularization is unnecessary in this case. 

However, when estimating \(\lambda\), \(\mu\), and \(\sigma\) simultaneously, the parameters can offset each other, and the correct ordering of the pure premium alone does not provide sufficient information to identify the parameters accurately. To address this, we run our ABC algorithm with the following settings:  
$$
J = 500, \quad R = 500, \quad \text{and} \quad \epsilon_{\min} = 10^{-8}.
$$
The prior distributions for the model parameters are defined as:  
$$
\lambda \sim \UnifDist([0, 10]), \quad \mu \sim \UnifDist([-3, 3]), \quad \text{and} \quad \sigma \sim \UnifDist([0, 2]).
$$
The resulting posterior distributions are provided in \cref{fig:posterior_commercial_premium_wo_reg}.

\begin{figure}[!ht]
  \begin{center}
    \subfloat[Posterior distribution of $\lambda$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_lambda_wo_reg_supp_mat}
      \label{sub:posterior_lambda_wo_reg_supp_mat}
    }
    \subfloat[Posterior distribution of $\mu$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_mu_wo_reg_supp_mat}
      \label{sub:posterior_mu_wo_reg_supp_mat}
    }\\
    \subfloat[Posterior distribution of $\sigma$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_sig_wo_reg_supp_mat}
      \label{sub:posterior_sig_wo_reg_supp_mat}
    }
    \caption{Posterior distributions of the parameters of the $\text{Pois}(\lambda)-\text{LogNormal}(\mu , \sigma)$ risk model.}
    \label{fig:posterior_commercial_premium_wo_reg}
  \end{center}
\end{figure}

Too many parameter combinations result in the correct ordering, making it difficult to identify the relevant portions of the parameter space. Regularization is essential to address this issue, as it allows us to exclude irrelevant regions. To this end, we define a loss ratio corridor with:  
$$
\text{LR}_{\text{low}} = 0.7 \quad \text{and} \quad \text{LR}_{\text{high}} = 0.9.
$$
The posterior distributions are shown in \cref{fig:posterior_commercial_premium_w_reg}.

\begin{figure}[!ht]
  \begin{center}
    \subfloat[Posterior distribution of $\lambda$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_lambda_w_reg_supp_mat}
      \label{sub:posterior_lambda_w_reg_supp_mat}
    }
    \subfloat[Posterior distribution of $\mu$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_mu_w_reg_supp_mat}
      \label{sub:posterior_mu_w_reg_supp_mat}
    }\\
    \subfloat[Posterior distribution of $\sigma$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_sig_w_reg_supp_mat}
      \label{sub:posterior_sig_w_reg_supp_mat}
    }
    \caption{Posterior distributions of the parameters of the $\text{Pois}(\lambda)-\text{LogNormal}(\mu , \sigma)$ risk model.}
    \label{fig:posterior_commercial_premium_w_reg}
  \end{center}
\end{figure}
The posterior do not center arround the true value but at least the true values belongs to the set of possible values. The posterior distributions are informative despise the fact that we have only used three commercial rates. 

\subsection{Misspecified safety loading} \label{ssec:miss_specfifed_safety_loading}
Consider the following loading functions:  
$$
f_1(x) = 1.1 \cdot x, \quad f_2(x) = 1.2 \cdot x, \quad \text{and} \quad f_3(x) = 1.3 \cdot x.
$$
In this case, the isotonic model is misspecified because the pure premium and commercial premiums are not ordered consistently. As a result, the isotonic regression approximates the data points through interpolation, as illustrated in \cref{fig:pp_cp_miss_specified}.

\begin{figure}[!ht]
  \begin{center}
  \includegraphics[width=0.7\linewidth]{../figures/pp_cp_miss_specified}
    \caption{Isotonic fit of the pure and commercial premium when the isotonic model is misspecified.}
    \label{fig:pp_cp_miss_specified}
  \end{center}
\end{figure}

\cref{fig:rmse_sigma_iso_linear_miss} shows the plot of the $\sigma\mapsto  \text{RMSE}\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right]\Big\rvert_{\lambda =3, \mu =0}$ when $f$ is isotonic (\cref{sub:rmse_sigma_iso_miss}) and when $f$ is linear (\cref{sub:rmse_sigma_linear_miss}).

\begin{figure}[!ht]
  \begin{center}
    \subfloat[$f$ is isotonic]{
      \includegraphics[width=0.4\textwidth]{../figures/rmse_sigma_iso_miss}
      \label{sub:rmse_sigma_iso_miss}
    }
    \subfloat[$f$ is linear]{
      \includegraphics[width=0.4\textwidth]{../figures/rmse_sigma_linear_miss}
      \label{sub:rmse_sigma_linear_miss}
    }
    \caption{$\sigma\mapsto  \text{RMSE}\left[\Tilde{p}_{1:n}, f(p_{1:n}^\theta)\right]\Big\rvert_{\lambda =3, \mu =0}$.}
    \label{fig:rmse_sigma_iso_linear_miss}
  \end{center}
\end{figure}

The isotonic condition does not hold for the true parameter, and the RMSE of the isotonic model is minimized for incorrect values. Similarly, the linear fit is also inaccurate, just like in \cref{fig:pp_cp_well_specified}. 

We run our ABC algorithm with the following settings:  
$$
J = 500, \quad R = 500, \quad \text{and} \quad \epsilon_{\min} = 0.01.
$$
The prior distributions for the model parameters are defined as:  
$$
\lambda \sim \UnifDist([0, 10]), \quad \mu \sim \UnifDist([-3, 3]), \quad \text{and} \quad \sigma \sim \UnifDist([0, 2]).
$$
The resulting posterior distributions are provided in \cref{fig:posterior_commercial_premium_wo_reg_miss}. Note that regularization terms are not included at this stage.


\begin{figure}[!ht]
  \begin{center}
    \subfloat[Posterior distribution of $\lambda$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_lambda_wo_reg_miss_supp_mat}
      \label{sub:posterior_lambda_wo_reg_miss_supp_mat}
    }
    \subfloat[Posterior distribution of $\mu$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_mu_wo_reg_miss_supp_mat}
      \label{sub:posterior_mu_wo_reg_miss_supp_mat}
    }\\
    \subfloat[Posterior distribution of $\sigma$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_sig_wo_reg_miss_supp_mat}
      \label{sub:posterior_sig_wo_reg_miss_supp_mat}
    }
    \caption{Posterior distributions of the parameters of the $\text{Pois}(\lambda)-\text{LogNormal}(\mu , \sigma)$ risk model.}
    \label{fig:posterior_commercial_premium_wo_reg_miss}
  \end{center}
\end{figure}

The posterior distribution is useless. We run again our ABC algorithm, this time setting a loss ratio corridor with 
$$
\text{LR}_{\text{low}} = 0.76\text{, and }\text{LR}_{\text{high}} = 0.9. 
$$
The posterior distributions are provided on \cref{fig:posterior_commercial_premium_w_reg_miss}. 
\begin{figure}[!ht]
  \begin{center}
    \subfloat[Posterior distribution of $\lambda$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_lambda_w_reg_miss_supp_mat}
      \label{sub:posterior_lambda_w_reg_miss_supp_mat}
    }
    \subfloat[Posterior distribution of $\mu$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_mu_w_reg_miss_supp_mat}
      \label{sub:posterior_mu_w_reg_miss_supp_mat}
    }\\
    \subfloat[Posterior distribution of $\sigma$]{
      \includegraphics[width=0.4\textwidth]{../figures/posterior_sig_w_reg_miss_supp_mat}
      \label{sub:posterior_sig_w_reg_miss_supp_mat}
    }
    \caption{Posterior distributions of the parameters of the $\text{Pois}(\lambda)-\text{LogNormal}(\mu , \sigma)$ risk model.}
    \label{fig:posterior_commercial_premium_w_reg_miss}
  \end{center}
\end{figure}

The posterior distributions are slightly more informative now. However, using only three rates appears insufficient to achieve reliable results. The simulation study and the real data analysis in our paper consider scenarios with more than 25 rates and only two parameters to estimate.


\bibliographystyle{plainnat}
\bibliography{abc_no_data}


\end{document}